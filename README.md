<div align="center">
<h1> Image-Captioning
</h1>

<p>
Given the image as an input, it tries to generate the descrption of the image.
</p>

<p align="center">
   <img src="https://skillicons.dev/icons?i=python,github,tensorflow" />
</p>
<hr>

![download](https://user-images.githubusercontent.com/62739618/229368753-f8dc5a8e-cd5d-44e5-8953-55e474dcc0dc.png)


</div>

## Dataset:

> The Flickr_8K dataset is used for the model training of image caption generators.

## 1 .Basic Model Inference(CNN, LSTM):

> It includes the labeling of an image with English keywords with the help of datasets provided during model training. Imagenet dataset is used to train the CNN model called Xception. Xception is responsible for image feature extraction. These extracted features will be fed to the LSTM model which in turn generates the image caption.

![image](https://user-images.githubusercontent.com/62739618/229368911-0c90e3ac-5629-4dd2-851d-b3caf84c0713.png)
![image](https://user-images.githubusercontent.com/62739618/229368914-c9f511de-d570-419a-b5af-c01851747853.png)

## 2 .Improvised Model (Transformers):

> The Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer architecture allows for significantly more parallelization and can reach new state of the art results in translation quality.

![image](https://user-images.githubusercontent.com/62739618/229369056-15c33ee5-4377-416b-9c1e-daccc3d76799.png)


___
## Team Members:

```
19PD01 : Mathanamathav A S
19PD31 : Selva k
```
